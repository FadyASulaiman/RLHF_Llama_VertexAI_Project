{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating The tuned LLama model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating RLHF tuned models is still an active area of research, but training curves and side-by-side have yielded the most promising results in estimating RLHF gain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For evaluating we can look at different parameters, an example of those include:\n",
    "- Training Curves:\n",
    "    - rank_loss\n",
    "    - reward\n",
    "    - kl_loss\n",
    "- Automation Metrics:\n",
    "    - Classification: Micro/Macro F1\n",
    "    - Summarization: ROUGE-L\n",
    "    - Text Generation: BLEU, ROUGE-L\n",
    "- Side by Side Evaluation:\n",
    "    - Can be done manually\n",
    "    - Can be automated: Auto SxS; using an arbiter moder instead of a human labeler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd # For side-by-side evaluation\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore results with Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "directory named \"reward-logs\" has been set up to contain training logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!ls reward-logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next command launches tensorboard and outputs the curves and analytics deduced from our log file related to the reward model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir reward-logs --bind_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next command launches tensorboard and outputs the curves and analytics deduced from our log file related to the reinforcer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir reinforcer-logs --bind_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side-to-Side Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load tuned results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_tuned_path = 'eval_results_tuned.jsonl'\n",
    "eval_data_tuned = []\n",
    "\n",
    "with open(eval_tuned_path) as f:\n",
    "    for line in f:\n",
    "        eval_data_tuned.append(json.loads(line))\n",
    "\n",
    "print(eval_data_tuned[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load untuned results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_untuned_path = 'eval_results_untuned.jsonl'\n",
    "eval_data_untuned = []\n",
    "\n",
    "with open(eval_untuned_path) as f:\n",
    "    for line in f:\n",
    "        eval_data_untuned.append(json.loads(line))\n",
    "\n",
    "print(eval_data_untuned[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augmenting results side by side in a PD dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [sample['inputs']['inputs_pretokenized']\n",
    "           for sample in eval_data_tuned]\n",
    "\n",
    "untuned_completions = [sample['prediction']\n",
    "                       for sample in eval_data_untuned]\n",
    "\n",
    "tuned_completions = [sample['prediction']\n",
    "                     for sample in eval_data_tuned]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(\n",
    "    data={'prompt': prompts,\n",
    "          'base_model':untuned_completions,\n",
    "          'tuned_model': tuned_completions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kkbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
